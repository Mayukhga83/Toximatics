{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f6f18-7cf7-4492-a70f-a69d23ab0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########------------------------------MAIN FUNCTIONS FOR GENERATIONS---------------------------#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebf1a8e-b859-4a78-9d38-a45f9b36e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------- Prompt Creaters ---------------------------------------\n",
    "# Please create your own prompts if you want to work with other models \n",
    "# See the models prompting/input template and modify teh zeroshot and fewshot functions  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1b2f64-37f9-4ed8-a691-3c6c28860308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction_prompt(utterance, context = 'contextual scenario', qualifier = 'toxic'):\n",
    "    \n",
    "    '''\n",
    "    wraps the utterance with prompt and qualifier\n",
    "    '''\n",
    "    prompt = 'Add ' + context + ' to the utterance ' + '\\''+ utterance + '\\'' + ', such that the statement becomes ' + qualifier\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0470b0-be66-45ef-9b40-25e8dbccffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_zeroshot_llama2(instruction):\n",
    "    '''\n",
    "    zero shot prompting in llama_2 style\n",
    "    '''\n",
    "    system_prompt = \"### System:\\nYou are an assistance which adds context to any given utterance.\\n\\n\"\n",
    "    prompt = f\"{system_prompt}### User: {instruction}\\n\\n### Assistant:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506f6589-1e6c-4f91-944a-24129c0eae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_fewshot_llama2(utterance, num_shot, examples,context = 'contextual scenario', qualifier= 'toxic'):\n",
    "    '''\n",
    "    Few-shot prompting in llama-2 style\n",
    "    Args:\n",
    "    utterance :str       any utterance or hate speech\n",
    "    num_shots   :int      number of shots  \n",
    "    examples    :list     list of list containing examples of utterance and context  len(examples) == num_shots \n",
    "    '''\n",
    "    assert len(examples) == num_shot, 'Mismatch num-shots and number of examples'\n",
    "    system_prompt = \"### System:\\nYou are an assistance which adds context to any given utterance.\\n\\n\"\n",
    "    prompt = f\"{system_prompt}\"\n",
    "    for item in examples:\n",
    "        add = f\"### User: {instruction_prompt(item[0], context=context, qualifier = qualifier)}\\n\\n### Assistant:{item[1]}\\n\\n\"\n",
    "        prompt = prompt + add\n",
    "\n",
    "    input = f\"### User: {instruction_prompt(utterance, context=context, qualifier = qualifier)}\\n\\n### Assistant:\"\n",
    "    prompt = prompt + input\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be00dea-2b2c-4480-bc85-6421f381abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_utterance(context):\n",
    "    '''\n",
    "    prompt to generate utterance for multistage generation\n",
    "    '''\n",
    "    prompt = 'Add utterance to the context ' + '\\''+ context + '\\'' \n",
    "        \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def prompt_zeroshot_utterance(instruction):\n",
    "    system_prompt = \"### System:\\nYou are a dialog writer, who can write dialog for any given contextual scenario such that the final sentence follows the context\\n\\n\"\n",
    "    prompt = f\"{system_prompt}### User: {instruction}\\n\\n### Assistant:\"\n",
    "    return prompt\n",
    "\n",
    "def prompt_fewshot_utternace(context, num_shot, examples):\n",
    "    '''\n",
    "    Args:\n",
    "    num_shots   :int       number of shots  \n",
    "    examples    :list      list containing examples of utterance and context  len(examples) == num_shots \n",
    "    '''\n",
    "    assert len(examples) == num_shot, 'Mismatch num-shots and number of examples'\n",
    "    system = \"### System:\\nYou are a dialog writer, who can write dialog for any given contextual scenario such that the final sentence follows the context\\n\\n\"\n",
    "    prompt = system + ''\n",
    "    for item in examples:\n",
    "        add = f\"### User: {prompt_utterance(item[0])}\\n\\n### Assistant:{item[1]}\\n\\n\"\n",
    "        prompt +=add\n",
    "\n",
    "    prompt = prompt + f\"### User: {prompt_utterance(context)}\\n\\n### Assistant:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426b0a44-87c5-438f-9037-f7eff23aa003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------- Error Handling & Incontext Example Function ---------------------------------------\n",
    "# These functions does some error handling for the generation functions \n",
    "# It also pulls incontext examples from the Example folder according to the given polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea904d1a-fddf-4ea6-be31-39f3f7468c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_assertion(num_shot_con1, num_shot_con2, num_shot_utt, in_p, tran_p, out_p, double_polarity=False ):\n",
    "    examples = examples_pull(in_p=in_p, tran_p= tran_p, out_p= out_p, double_polarity=double_polarity)\n",
    "    \n",
    "    assert num_shot_con1 <= len(examples[0]), 'AssertionError: num_shot_con1 greater than examples: decrease num_shot_con1 or increase examples in examples file'\n",
    "    assert num_shot_con2 <= len(examples[1]), 'AssertionError: num_shot_con2 greater than examples: decrease num_shot_con2 or increase examples in examples file'    \n",
    "    assert num_shot_utt <= len(examples[2]),  'AssertionError: num_shot_utt greater than examples: decrease num_shot_utt or increase examples in examples file'\n",
    "    examples_con1 = examples[0][:num_shot_con1]\n",
    "    examples_utt = examples[1][:num_shot_utt]\n",
    "    if double_polarity == True:\n",
    "        example_double_toxic = examples[2][:num_shot_con2]\n",
    "        example_double_benign = examples[3][:num_shot_con2]\n",
    "        return examples_con1, examples_utt, examples_double_toxic, examples_double_benign \n",
    "    else:\n",
    "        \n",
    "        examples_con2 = examples[2][:num_shot_con2]\n",
    "        return examples_con1, examples_utt, examples_con2\n",
    "    \n",
    "polarity = ['benign', 'toxic', 'neutral']\n",
    "\n",
    "def examples_pull(in_p, tran_p, out_p, double_polarity = False):\n",
    "    assert in_p in polarity, 'Assertion error: Invalid polarity for in_p'\n",
    "    assert tran_p in polarity, 'Assertion error: Invalid polarity for tran_p'\n",
    "    assert out_p in polarity, 'Assertion error: Invalid polarity for out_p '\n",
    "    examples =  examples_file(in_p, tran_p, out_p, double_polarity = double_polarity)\n",
    "    with open(f\"Examples/{examples[0]}\") as fp:   \n",
    "        examples_con1 = json.load(fp)\n",
    "    with open(f\"Examples/{examples[1]}\") as fp:   \n",
    "        examples_utt = json.load(fp)\n",
    "    \n",
    "    if double_polarity == True:\n",
    "        with open(f\"Examples/{examples[2]}\") as fp:   \n",
    "            examples_double_toxic = json.load(fp)\n",
    "        with open(f\"Examples/{examples[3]}\") as fp:   \n",
    "            examples_double_benign = json.load(fp)\n",
    "        return examples_con1, examples_utt, examples_double_toxic, examples_double_benign \n",
    "    else:\n",
    "        with open(f\"Examples/{examples[2]}\") as fp:   \n",
    "            examples_con2 = json.load(fp)\n",
    "        return examples_con1, examples_utt, examples_con2 \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "def examples_file(in_p, tran_p, out_p, double_polarity = False):\n",
    "    examples_con1 = in_p + '_' + tran_p + '_examples' + '.json'\n",
    "    examples_utt = 'examples_utterance.json'\n",
    "    if double_polarity == True:\n",
    "        \n",
    "        examples_double_toxic = 'neutral_' + 'toxic' + '_examples' + '.json'\n",
    "        examples_double_benign = 'neutral_' + 'benign' + '_examples' + '.json'\n",
    "        return examples_con1, examples_utt, examples_double_toxic, examples_double_benign\n",
    "    else:\n",
    "        examples_con2 = 'neutral_' + out_p + '_examples' + '.json'\n",
    "        return examples_con1, examples_utt, examples_con2\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adde8ed2-cbbb-42a7-9694-f7ce6b896717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_assertion_direct(num_shot, in_p, out_p, double_polarity=False ):\n",
    "    \n",
    "    examples = examples_pull_direct(in_p, out_p, double_polarity = False)\n",
    "    \n",
    "    for item in examples:\n",
    "        assert num_shot <= len(item), 'AssertionError: num_shot greater than examples: decrease num_shot or increase examples in examples file'\n",
    "    if double_polarity == True:\n",
    "        example_double_toxic = examples[0][:num_shot]\n",
    "        example_double_benign = examples[1][:num_shot]\n",
    "        return example_double_toxic, example_double_benign\n",
    "    else:\n",
    "        \n",
    "        examples_trans = examples[0][:num_shot]\n",
    "        return examples_trans\n",
    "    \n",
    "polarity = ['benign', 'toxic', 'neutral']\n",
    "\n",
    "def examples_pull_direct(in_p, out_p, double_polarity = False):\n",
    "    assert in_p in polarity, 'Assertion error: Invalid polarity for in_p'\n",
    "    assert out_p in polarity, 'Assertion error: Invalid polarity for out_p '\n",
    "    examples =  examples_file_direct(in_p=in_p, out_p=out_p, double_polarity = double_polarity)\n",
    "    \n",
    "    if double_polarity == True:\n",
    "        with open(f\"Examples/{examples[0]}\") as fp:   \n",
    "            examples_double_toxic = json.load(fp)\n",
    "        with open(f\"Examples/{examples[1]}\") as fp:   \n",
    "            examples_double_benign = json.load(fp)\n",
    "        return examples_double_toxic, examples_double_benign\n",
    "    else:\n",
    "        with open(f\"Examples/{examples[0]}\") as fp:   \n",
    "            examples_tran = json.load(fp)\n",
    "        return [examples_tran]\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "def examples_file_direct(in_p, out_p, double_polarity = False):\n",
    "    examples_tran = in_p + '_' + out_p + '_examples' + '.json'\n",
    "    if double_polarity == True:\n",
    "        \n",
    "        examples_tran_1 = in_p + '_toxic' + '_examples' + '.json'\n",
    "        examples_tran_2 = in_p + '_benign' + '_examples' + '.json'\n",
    "        return examples_tran_1, examples_tran_2\n",
    "    return [examples_tran]\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc1964c-d937-4b7a-a573-7b0c171db8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  For editing utterance: Has not been used in the paper\n",
    "polarity = ['benign', 'toxic', 'neutral']\n",
    "def fewshot_assertion_edit(num_shot, in_p, out_p):\n",
    "    \n",
    "    examples = examples_pull_direct_edit(in_p, out_p)\n",
    "    \n",
    "    for item in examples:\n",
    "        assert num_shot <= len(item), 'AssertionError: num_shot greater than examples: decrease num_shot or increase examples in examples file'\n",
    "    return examples[0][:num_shot]\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "def examples_pull_edit(in_p, out_p):\n",
    "    examples =  examples_file_direct_edit(in_p=in_p, out_p=out_p)\n",
    "    \n",
    "    with open(f\"Examples/{examples}\") as fp:   \n",
    "        examples = json.load(fp)\n",
    "        \n",
    "    return [examples]\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "def examples_file_edit(in_p, out_p):\n",
    "    assert in_p in polarity, 'Assertion error: Invalid polarity for in_p'\n",
    "    assert out_p in polarity, 'Assertion error: Invalid polarity for out_p '\n",
    "    \n",
    "    return in_p + '_' + out_p + '_edit_examples' + '.json'\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e2589e-62c9-494c-ad73-9db53ca0e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------- Generation Function ---------------------------------------\n",
    "# These are the base functions for the generation methods \n",
    "# Resemblance to paper: augment_direct--> Direct Augment\n",
    "# Resemblance to paper: augment_multistage--> Multistage Augment\n",
    "# Resemblance to paper: augment_niter_multistage--> N-iter Multistage Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6701efe-6bbf-4c68-880c-9fe48d751bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_augment_generator(utterance,  model, tokenizer, num_shot, examples, bad_words_ids, context = 'contextual scenario', qualifier='toxic', do_sample=True, penalty_alpha=0.6, top_k=8, max_new_tokens=70, repetition_penalty=1.2):\n",
    "        prompt = prompt_fewshot_llama2(utterance, num_shot, examples, qualifier='toxic')   \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        out_list = [utterance]\n",
    "        tokens = model.generate(**inputs,  do_sample=do_sample, penalty_alpha=penalty_alpha, top_k=top_k,  max_new_tokens= max_new_tokens, repetition_penalty=repetition_penalty, bad_words_ids=bad_words_ids)\n",
    "        out_list.append(tokenizer.decode(tokens[0]))         \n",
    "        return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ccefcf4-75c9-4195-b21b-4983b39218d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_multistage(lis, assertion = True):\n",
    "    generation = lis[1].split('### Assistant:')\n",
    "    if assertion == True:\n",
    "        \n",
    "        try:\n",
    "            assert len(generation) >= 8\n",
    "            if len(generation) == 8:\n",
    "                extract = generation[-1]\n",
    "            elif len(generation) > 8:\n",
    "                extract = generation[7]\n",
    "            \n",
    "        except AssertionError:\n",
    "            print('\\n model generation was erroneous')\n",
    "            extract = 'N/A'\n",
    "    generation = extract.split('<eos>')\n",
    "    generation = generation[0]\n",
    "    generation = generation.replace('<s>', '')\n",
    "    generation = generation.replace('</s>', '')\n",
    "    generation = generation.replace('eos', '')\n",
    "    generation = generation.replace('<', '')\n",
    "    generation = generation.replace('>', '')\n",
    "    return generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7d413a-b56a-4a8d-b810-76aa0c43fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Augment Method\n",
    "# inputs: utterance--> str, model--> transformers model, tokenizer--> transformer tokenizer \n",
    "# num_shot--> int (number of in context example, default 6: changing this requires changing the number of examples in examples folder)\n",
    "# in_p--> str (input polarity e.g. 'toxic', 'benign'), out_p--> desired output polarity\n",
    "# bad_words_ids--> list[int] (ids to omit, default provided as bag_of_words_ids.json)\n",
    "# do_sample=True, penalty_alpha=0.6, top_k=8 --> Contrastive Decoding Parameters\n",
    "# double_polarity--> Boolean (if True will generate both toxic and benign context for the given utterance) \n",
    "\n",
    "\n",
    "def augment_direct(utterance,  model, tokenizer, num_shot, in_p, out_p, bad_words_ids, context = 'contextual scenario', qualifier='toxic', do_sample=True, penalty_alpha=0.6, top_k=8, max_new_tokens=70, repetition_penalty=1.2, double_polarity=False):\n",
    "       \n",
    "\n",
    "    examples = fewshot_assertion_direct(num_shot=num_shot, in_p=in_p, out_p=out_p, double_polarity=double_polarity)\n",
    "    \n",
    "    \n",
    "    if double_polarity == True:\n",
    "        context_augment = direct_augment_generator(utterance, model=model, \n",
    "                                              tokenizer=tokenizer, qualifier='toxic', \n",
    "                                              bad_words_ids=bad_words_ids, num_shot=num_shot, \n",
    "                                              examples=examples[0], penalty_alpha=penalty_alpha, \n",
    "                                              top_k=top_k, max_new_tokens=max_new_tokens)\n",
    "    \n",
    "        extract_context_toxic = extract_multistage(context_augment)\n",
    "        context_augment = direct_augment_generator(utterance, model=model, \n",
    "                                              tokenizer=tokenizer, qualifier='benign', \n",
    "                                              bad_words_ids=bad_words_ids, num_shot=num_shot, \n",
    "                                              examples=examples[1], penalty_alpha=penalty_alpha, \n",
    "                                              top_k=top_k, max_new_tokens=max_new_tokens)\n",
    "    \n",
    "        extract_context_benign = extract_multistage(context_augment)\n",
    "        return [{'utterance':utterance,'context_toxic': extract_context_toxic}, {'utterance':utterance,'context_benign': extract_context_benign}]     \n",
    "    \n",
    "    context_augment = direct_augment_generator(utterance, model=model, \n",
    "                                              tokenizer=tokenizer, qualifier=qualifier, \n",
    "                                              bad_words_ids=bad_words_ids, num_shot=num_shot, \n",
    "                                              examples=examples, penalty_alpha=penalty_alpha, \n",
    "                                              top_k=top_k, max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    extract_context = extract_multistage(context_augment)\n",
    "    \n",
    "    return [{'utterance':utterance,'context': extract_context}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f1ced8e-6ccb-4a56-a244-d8992511a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Utterance generation functions for Multistage Generations\n",
    "\n",
    "def generate_utterance_fewshot(context, utterance, model, tokenizer, num_shot, examples, do_sample=True, penalty_alpha=0.6, top_k=8, max_new_tokens=40, repetition_penalty=1.2):     \n",
    "    prompt = prompt_fewshot_utternace(context,num_shot, examples)        \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out_list = []\n",
    "    tokens = model.generate(**inputs,  do_sample=do_sample, penalty_alpha=penalty_alpha, top_k=top_k,  max_new_tokens= max_new_tokens, repetition_penalty=repetition_penalty)\n",
    "    out_list.append(context)\n",
    "    out_list.append(tokenizer.decode(tokens[0])) \n",
    "    return out_list    \n",
    "\n",
    "\n",
    "def generate_utterance_zeroshot(context, utterance, model, tokenizer, do_sample=True, penalty_alpha=0.6, top_k=8, max_new_tokens=40, repetition_penalty=1.2):     \n",
    "    prompt = prompt_utterance(context)\n",
    "    prompt = prompt_zeroshot_utterance(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out_list = []\n",
    "    tokens = model.generate(**inputs,  do_sample=do_sample, penalty_alpha=penalty_alpha, top_k=top_k,  max_new_tokens= max_new_tokens, repetition_penalty=repetition_penalty)\n",
    "    out_list.append(context)\n",
    "    out_list.append(tokenizer.decode(tokens[0])) \n",
    "    return out_list  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf89be7-4fe4-4b33-8420-7fc69dcd1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multistage Augment Method\n",
    "# inputs: utterance--> str, model_con1, model_con2, model_utt --> transformers model,\n",
    "# tokenizer_con1, tokenizer_con2, tokenizer_utt--> transformer tokenizer\n",
    "# num_shot--> int (number of in context example, default 6: changing this requires changing the number of examples in examples folder)\n",
    "# in_p--> str (input polarity e.g. 'toxic', 'benign'), out_p--> desired output polarity\n",
    "# bad_words_ids--> list[int] (ids to omit, default provided as bag_of_words_ids.json)\n",
    "# do_sample=True, pa_con1=0.6, topk_con1=8, pa_con2=0.6, topk_con2=8, pa_utt=0.6, topk_utt=8--> Contrastive Decoding Parameters\n",
    "# double_polarity--> Boolean (if True will generate both toxic and benign context for the given utterance) \n",
    "\n",
    "\n",
    "\n",
    "def augment_multistage(utterance, model_con1, model_con2, model_utt, \n",
    "                                 tokenizer_con1, tokenizer_con2, tokenizer_utt, bad_words_ids,\n",
    "                                 in_p='neutral', tran_p ='benign', out_p= 'toxic',\n",
    "                                 num_shot_con1= 6, num_shot_con2=6, num_shot_utt=6,\n",
    "                                 context = 'contextual scenario',  \n",
    "                                 qualifier_con1='highly positive', qualifier_con2='offensive',\n",
    "                                 do_sample=True, pa_con1=0.6, topk_con1=8, pa_con2=0.6, topk_con2=8, pa_utt=0.6, topk_utt=8, \n",
    "                                 maxtoken_con1=70, maxtoken_con2 = 70, maxtoken_utt=40, repetition_penalty=1.2, double_polarity= False, multiple_utternace = False, multiple_utternace_no = None):\n",
    "\n",
    "   \n",
    "    if multiple_utternace == True: assert multiple_utternace_no > 1, 'Please add the number of utterance example with multiple_utternace_no '\n",
    "    examples = fewshot_assertion(num_shot_con1=num_shot_con1, num_shot_con2=num_shot_con2, \n",
    "                                                                   num_shot_utt=num_shot_utt, in_p=in_p, tran_p=tran_p, \n",
    "                                                                   out_p=out_p, double_polarity=double_polarity )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##  context -- generation\n",
    "    context_augment = direct_augment_generator(utterance, model=model_con1, \n",
    "                                              tokenizer=tokenizer_con1, qualifier=qualifier_con1, \n",
    "                                              bad_words_ids=bad_words_ids, num_shot=6, \n",
    "                                              examples=examples[0], penalty_alpha=pa_con1, \n",
    "                                              top_k=topk_con1, max_new_tokens=maxtoken_con1)\n",
    "    ##  context -- extraction\n",
    "    extract_context = extract_multistage(context_augment)\n",
    "    \n",
    "    if multiple_utternace == True:\n",
    "        multiple_utt = []\n",
    "        for i in range(multiple_utternace_no):\n",
    "            utt_generate = generate_utterance_fewshot(context=extract_context, utterance=utterance,\n",
    "                                       model=model_utt, tokenizer=tokenizer_utt, examples=examples[1], num_shot=6,  \n",
    "                                        penalty_alpha=pa_utt,\n",
    "                                       top_k=topk_utt, max_new_tokens=maxtoken_utt)\n",
    "            utt_ex = extract_multistage(utt_generate, assertion=True)\n",
    "            multiple_utt.append(utt_ex)\n",
    "    \n",
    "        out = {}\n",
    "        if double_polarity == True:\n",
    "            for item in multiple_utt:\n",
    "            \n",
    "                context_augment_2 = direct_augment_generator(item, model=model_con2, \n",
    "                                              tokenizer=tokenizer_con2, qualifier='toxic', \n",
    "                                              bad_words_ids=bad_words_ids, num_shot=6, \n",
    "                                              examples=examples[2], penalty_alpha=pa_con2, \n",
    "                                              top_k=topk_con2, max_new_tokens=maxtoken_con2)\n",
    "                context_augment_3 = direct_augment_generator(item, model=model_con2, \n",
    "                                                  tokenizer=tokenizer_con2, qualifier='benign', \n",
    "                                                  bad_words_ids=bad_words_ids, num_shot=6, \n",
    "                                                  examples=examples[3], penalty_alpha=pa_con2,\n",
    "                                                  top_k=topk_con2, max_new_tokens=maxtoken_con2)\n",
    "   \n",
    "                extract_context_2 = extract_multistage(context_augment_3)\n",
    "                extract_context_3 = extract_multistage(context_augment_3)\n",
    "                out['utterance_' + multiple_utt.index(item)] = [(item, extract_context_2), (item, extract_context_3) ]\n",
    "        else:   \n",
    "            for item in multiple_utt:\n",
    "                \n",
    "                context_augment_2 = direct_augment_generator(utt_ex, model=model_con2, \n",
    "                                                  tokenizer=tokenizer_con2, qualifier=qualifier_con2, \n",
    "                                                  bad_words_ids=bad_words_ids, num_shot=6, \n",
    "                                                  examples=examples[2], penalty_alpha=pa_con2, \n",
    "                                                  top_k=topk_con2, max_new_tokens=maxtoken_con2)\n",
    "            \n",
    "                extract_context_2 = extract_multistage(context_augment_2)\n",
    "                out['utterance_' + multiple_utt.index(item)] = [item, extract_context_2]\n",
    "        return out\n",
    "    \n",
    "    else:\n",
    "        utt_generate = generate_utterance_fewshot(context=extract_context, utterance=utterance,\n",
    "                                       model=model_utt, tokenizer=tokenizer_utt, examples=examples[1], num_shot=6,  \n",
    "                                        penalty_alpha=pa_utt,\n",
    "                                       top_k=topk_utt, max_new_tokens=maxtoken_utt)\n",
    "        utt_ex = extract_multistage(utt_generate, assertion=True)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if double_polarity == True:\n",
    "        \n",
    "        ##  pool neutral to toxic examples\n",
    "        context_augment_2 = direct_augment_generator(utt_ex, model=model_con2, \n",
    "                                              tokenizer=tokenizer_con2, qualifier='toxic', \n",
    "                                              bad_words_ids=bad_words_ids, num_shot=6, \n",
    "                                              examples=examples[2], penalty_alpha=pa_con2, \n",
    "                                              top_k=topk_con2, max_new_tokens=maxtoken_con2)\n",
    "        context_augment_3 = direct_augment_generator(utt_ex, model=model_con2, \n",
    "                                              tokenizer=tokenizer_con2, qualifier='benign', \n",
    "                                              bad_words_ids=bad_words_ids, num_shot=6, \n",
    "                                              examples=examples[3], penalty_alpha=pa_con2, \n",
    "                                              top_k=topk_con2, max_new_tokens=maxtoken_con2)\n",
    "    ##  2 context -- extraction\n",
    "        extract_context_2 = extract_multistage(context_augment_3)\n",
    "        extract_context_3 = extract_multistage(context_augment_3)\n",
    "        return [{'utterance':utt_ex,'context_toxic': extract_context_2}, {'utterance':utt_ex,'context_benign': extract_context_3} ]\n",
    "        \n",
    "    context_augment_2 = direct_augment_generator(utt_ex, model=model_con2, \n",
    "                                              tokenizer=tokenizer_con2, qualifier=qualifier_con2, \n",
    "                                              bad_words_ids=bad_words_ids, num_shot=6, \n",
    "                                              examples=examples[2], penalty_alpha=pa_con2, \n",
    "                                              top_k=topk_con2, max_new_tokens=maxtoken_con2)\n",
    "    ##  2 context -- extraction\n",
    "    extract_context_2 = extract_multistage(context_augment_2)\n",
    "    \n",
    "    return [{'utterance':utt_ex,'context': extract_context_2}]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d0715d3-07f2-4b6f-b94b-610a2bdc8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-iteration Multistage Augment Method\n",
    "# inputs: n--> int (number of iteration), utterance--> str, model_con1, model_con2, model_utt --> transformers model,\n",
    "# tokenizer_con1, tokenizer_con2, tokenizer_utt--> transformer tokenizer\n",
    "# stack_output--> Boolean (if True then output will contain intermediate generations)\n",
    "# num_shot--> int (number of in context example, default 6: changing this requires changing the number of examples in examples folder)\n",
    "# in_p--> str (input polarity e.g. 'toxic', 'benign'), out_p--> desired output polarity\n",
    "# bad_words_ids--> list[int] (ids to omit, default provided as bag_of_words_ids.json)\n",
    "# do_sample=True, pa_con1=0.6, topk_con1=8, pa_con2=0.6, topk_con2=8, pa_utt=0.6, topk_utt=8--> Contrastive Decoding Parameters\n",
    "# double_polarity--> Boolean (if True will generate both toxic and benign context for the given utterance)\n",
    "\n",
    "\n",
    "def augment_niter_multistage(n,utterance, model_con1, model_con2, model_utt, \n",
    "                                 tokenizer_con1, tokenizer_con2, tokenizer_utt, bad_words_ids, stack_output = False,\n",
    "                                 in_p='neutral', tran_p ='benign', out_p= 'toxic',\n",
    "                                 num_shot_con1= 6, num_shot_con2=6, num_shot_utt=6,\n",
    "                                 context = 'contextual scenario',  \n",
    "                                 qualifier_con1='highly positive', qualifier_con2='offensive',\n",
    "                                 do_sample=True, pa_con1=0.6, topk_con1=8, pa_con2=0.6, topk_con2=8, pa_utt=0.6, topk_utt=8, \n",
    "                                 maxtoken_con1=70, maxtoken_con2 = 70, maxtoken_utt=40, repetition_penalty=1.2, double_polarity= False):\n",
    "   \n",
    "    \n",
    "    if double_polarity == True:\n",
    "        \n",
    "        ##  n-iter logic \n",
    "        if stack_output == True:\n",
    "                out = {}\n",
    "        for i in tqdm(range(n)):\n",
    "\n",
    "            ## generation logic\n",
    "            \n",
    "            \n",
    "                \n",
    "            epoch = augment_multistage(utterance, model_con1=model_con1, model_con2=model_con2, model_utt=model_utt, tokenizer_utt=tokenizer_utt, \n",
    "                             tokenizer_con1=tokenizer_con1, tokenizer_con2=tokenizer_con2,bad_words_ids=bad_words_ids, \n",
    "                             in_p=in_p, tran_p =tran_p, out_p= out_p,\n",
    "                             num_shot_con1= num_shot_con1, num_shot_con2=num_shot_con2, num_shot_utt=num_shot_utt,\n",
    "                             context = context,  \n",
    "                             qualifier_con1=qualifier_con1, qualifier_con2=qualifier_con2,\n",
    "                             do_sample=do_sample, pa_con1=pa_con1, topk_con1=topk_con1, pa_con2=pa_con2, topk_con2=topk_con2, pa_utt=pa_utt, topk_utt=topk_utt, \n",
    "                             maxtoken_con1=maxtoken_con1, maxtoken_con2 = maxtoken_con2, maxtoken_utt=maxtoken_utt, repetition_penalty=repetition_penalty, double_polarity= double_polarity)\n",
    "            \n",
    "            utterance = epoch[0][0]\n",
    "            if stack_output == True:\n",
    "                out['epoch' + str(i+1)] = epoch \n",
    "            \n",
    "\n",
    "            \n",
    "               \n",
    "            \n",
    "        return out if stack_output == True else epoch\n",
    "                \n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    out = {}\n",
    "    for i in tqdm(range(n)):\n",
    "\n",
    "        if stack_output == True:\n",
    "            \n",
    "            \n",
    "            epoch = augment_multistage(utterance, model_con1=model_con1, model_con2=model_con2, model_utt=model_utt, tokenizer_utt=tokenizer_utt, \n",
    "                             tokenizer_con1=tokenizer_con1, tokenizer_con2=tokenizer_con2,bad_words_ids=bad_words_ids, \n",
    "                             in_p=in_p, tran_p =tran_p, out_p= out_p,\n",
    "                             num_shot_con1= num_shot_con1, num_shot_con2=num_shot_con2, num_shot_utt=num_shot_utt,\n",
    "                             context = context,  \n",
    "                             qualifier_con1=qualifier_con1, qualifier_con2=qualifier_con2,\n",
    "                             do_sample=do_sample, pa_con1=pa_con1, topk_con1=topk_con1, pa_con2=pa_con2, topk_con2=topk_con2, pa_utt=pa_utt, topk_utt=topk_utt, \n",
    "                             maxtoken_con1=maxtoken_con1, maxtoken_con2 = maxtoken_con2, maxtoken_utt=maxtoken_utt, repetition_penalty=repetition_penalty, double_polarity= double_polarity)\n",
    "        \n",
    "            utterance = epoch[0]\n",
    "            out['epoch' + str(i+1)] = epoch\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        epoch = augment_multistage(utterance, model_con1=model_con1, model_con2=model_con2, model_utt=model_utt, tokenizer_utt=tokenizer_utt, \n",
    "                             tokenizer_con1=tokenizer_con1, tokenizer_con2=tokenizer_con2,bad_words_ids=bad_words_ids, \n",
    "                             in_p=in_p, tran_p =tran_p, out_p= out_p,\n",
    "                             num_shot_con1= num_shot_con1, num_shot_con2=num_shot_con2, num_shot_utt=num_shot_utt,\n",
    "                             context = context,  \n",
    "                             qualifier_con1=qualifier_con1, qualifier_con2=qualifier_con2,\n",
    "                             do_sample=do_sample, pa_con1=pa_con1, topk_con1=topk_con1, pa_con2=pa_con2, topk_con2=topk_con2, pa_utt=pa_utt, topk_utt=topk_utt, \n",
    "                             maxtoken_con1=maxtoken_con1, maxtoken_con2 = maxtoken_con2, maxtoken_utt=maxtoken_utt, repetition_penalty=repetition_penalty, double_polarity= double_polarity)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        utterance = epoch[0]['utterance']\n",
    "    \n",
    "    if stack_output==True:\n",
    "        return out\n",
    "    return epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fea3e-3692-48ce-88e7-2f69a7d2c238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9f73c7e-e3fc-44f8-adce-202cdccb5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########------------------------------ Using the Generation Methods Directly ---------------------------#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323910b-9b78-4911-9bf7-27e89fe4b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please install modules from requirements.txt before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5be058d3-66b8-45d3-9cd7-ef00a069056a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d512e4bdc0248a29f5f69f4bb821b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"quantumaikr/llama-2-70b-fb16-orca-chat-10k\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"quantumaikr/llama-2-70b-fb16-orca-chat-10k\", device_map =\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d4bf67f-c726-4eda-b513-ca9d06458d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "with open(\"bag_of_words_ids.json\", \"r\")as f:\n",
    "    bag_of_words_ids = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a28ebfd2-4595-4cb6-912f-5f370835fee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3533655-70c3-4aca-9869-1e2bce7a5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note: We are using the same model for multistage and N-iter multistage\n",
    "# If you wand to use multiple models, input the models in the correct order, see fuctions augment_multistage and augment_niter_multistage\n",
    "# For Multistage methods, if you want you can play with stack_output or multiple_utterance. multiple_utterance was not used in the paper due to high time complexity.\n",
    "# stack_output--> stack the intermediate utterance and context and show it in the output\n",
    "# multiple_utterance ---> generates many  alterante intermediate utterance instead of one utterance and context for all branched utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caec3662-c8fa-48a8-9c24-f80d9e65e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = \"I think we should meet at night \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a6db4bb-2d43-44dd-a0c0-473da8186ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'utterance': 'I think we should meet at night ', 'context': 'Imagine being in a workplace where sexual harassment is prevalent. A male coworker says, \"I think we should meet at night,\" implying an unwanted romantic encounter instead of discussing work matters during daytime hours.'}]\n"
     ]
    }
   ],
   "source": [
    "# Direct Augment\n",
    "generation = augment_direct(utterance,  model=model, tokenizer=tokenizer, num_shot=6, in_p='benign', out_p='toxic', bad_words_ids=bag_of_words_ids, context = 'contextual scenario', qualifier='toxic', do_sample=True, penalty_alpha=0.6, top_k=8, max_new_tokens=70, repetition_penalty=1.2, double_polarity=False)\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33c522cd-b7e0-48e1-9cf2-7af31b98b83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'utterance': ' \"Hey guys, why don\\'t we plan something fun like a happy hour where we can relax and talk about the latest developments? What do you say?\" ', 'context': 'Imagine a group project meeting where team members are feeling stressed out because of constant deadlines and conflicts arising among themselves. One member suggests having a casual gathering outside work hours to bond and share ideas informally. However, another colleague feels threatened by this suggestion since they believe it would lead to exclusion and prefer formal communication methods'}]\n"
     ]
    }
   ],
   "source": [
    "# Multistage Augment \n",
    "generation = augment_multistage(utterance, model, model, model, \n",
    "                                 tokenizer, tokenizer, tokenizer, bad_words_ids=bag_of_words_ids,\n",
    "                                 in_p='benign', tran_p ='toxic', out_p= 'benign',\n",
    "                                 num_shot_con1= 6, num_shot_con2=6, num_shot_utt=6,\n",
    "                                 context = 'contextual scenario',  \n",
    "                                 qualifier_con1='highly positive', qualifier_con2='offensive',\n",
    "                                 do_sample=True, pa_con1=0.6, topk_con1=8, pa_con2=0.6, topk_con2=8, pa_utt=0.6, topk_utt=8, \n",
    "                                 maxtoken_con1=70, maxtoken_con2 = 70, maxtoken_utt=40, repetition_penalty=1.2, double_polarity= False, multiple_utternace = False, multiple_utternace_no = None)\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "631bb535-433c-446a-acba-ab921576ae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 2/2 [03:44<00:00, 112.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'utterance': ' \"Let\\'s light up these firecrackers and see how crazy things get!\" ', 'context': 'During a political rally, an influential figure encourages supporters to use violence and intimidation tactics against opposing parties while promoting their own agenda, creating chaos and division within the country.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# N-iter Multistage Augment \n",
    "generation = augment_niter_multistage(2,utterance, model, model, model, \n",
    "                                 tokenizer, tokenizer, tokenizer, bad_words_ids=bag_of_words_ids, stack_output = False,\n",
    "                                 in_p='neutral', tran_p ='benign', out_p= 'toxic',\n",
    "                                 num_shot_con1= 6, num_shot_con2=6, num_shot_utt=6,\n",
    "                                 context = 'contextual scenario',  \n",
    "                                 qualifier_con1='highly positive', qualifier_con2='offensive',\n",
    "                                 do_sample=True, pa_con1=0.6, topk_con1=8, pa_con2=0.6, topk_con2=8, pa_utt=0.6, topk_utt=8, \n",
    "                                 maxtoken_con1=70, maxtoken_con2 = 70, maxtoken_utt=40, repetition_penalty=1.2, double_polarity= False)\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f755d-51fd-4f8a-94bf-4d39da8a28c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757bd811-6063-40f4-abf1-de30ef86b6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db5c2f-2483-4a90-a8d3-48999e2d193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########--------------------------- Codes for Using Toxigen as utterance with the generation methods ---------------------------#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3932622-de9d-421d-94de-6b2a7d16544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting index of datapoints according to some score filter on toxicity, intent, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb36ae5-38e5-4b4a-9cfb-821ed9372117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_toxic_index(dataset, toxic_lowerbound, toxic_upperbound):\n",
    "    '''\n",
    "    Filter the index for the opted toxicity threshold\n",
    "    \n",
    "    '''\n",
    "    toxic_list = []\n",
    "    for index, item in enumerate(dataset['toxicity_human']):\n",
    "        if item > toxic_lowerbound and item < toxic_upperbound:\n",
    "            toxic_list.append(index)\n",
    "    return toxic_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1b0a4-0013-409f-b08c-48019932a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intent_index(dataset, intent_lowerbound, intent_upperbound):\n",
    "    '''\n",
    "    Filter the index for the opted intent threshold\n",
    "    \n",
    "    '''\n",
    "    intent_list = []\n",
    "    for index, item in enumerate(dataset['intent']):\n",
    "        if item > intent_lowerbound and item < intent_upperbound:\n",
    "            intent_list.append(index)\n",
    "    return intent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851da8e3-aa11-45f5-a31c-4347c8eaacf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_framing_index(dataset, frame = \"disagreement\" ):\n",
    "    '''\n",
    "    Filter the index for the opted framing\n",
    "\n",
    "    '''\n",
    "    framing_list = []\n",
    "    for index, item in enumerate(dataset['framing']):\n",
    "        if item == frame:\n",
    "            framing_list.append(index)\n",
    "    return framing_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a2338-a3ae-430b-929e-4e72b34882f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset load \n",
    "# Please use huggingface auth token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bccb8b-ba8d-4b78-87f5-3d4c068c4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12960be4-b200-4ec8-8a90-e55790b49545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_train = load_dataset('skg/toxigen-data', split='train', use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef45c04-0e5f-4fbc-8abc-4f149c328fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c01d3c-6c82-4a0c-a1d3-92cdba62c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pipeline_augment_direct(dataset_index, dataset, model, tokenizer, bad_words_ids= bad_words_ids, num_shot=6, \n",
    "                                in_p='neutral', out_p='toxic', \n",
    "                                context = 'contextual scenario', qualifier='toxic', \n",
    "                                do_sample=True, penalty_alpha=0.6, top_k=8, \n",
    "                                max_new_tokens=70, repetition_penalty=1.2, \n",
    "                                double_polarity=False):\n",
    "    out = {}\n",
    "    data = dataset_index\n",
    "    for i in tqdm(range(len(data))):\n",
    "        key = data[i]\n",
    "        utterance = dataset[key]['text']\n",
    "        generation = augment_direct( utterance=utterance,  model=model, tokenizer=tokenizer, num_shot=num_shot, \n",
    "                                    in_p=in_p, out_p=out_p, bad_words_ids=bad_words_ids, \n",
    "                                    context = context, qualifier=qualifier, \n",
    "                                    do_sample=do_sample, penalty_alpha=penalty_alpha, top_k=top_k, \n",
    "                                    max_new_tokens=max_new_tokens, repetition_penalty=repetition_penalty, \n",
    "                                    double_polarity=double_polarity)\n",
    "         \n",
    "        out[key] = generation\n",
    "            \n",
    "    return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b082331-c046-4c2b-9c4f-16fc4b379adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pipeline_augment_multistage(dataset_index, dataset, model_con1, model_con2, model_utt, \n",
    "                                 tokenizer_con1, tokenizer_con2, tokenizer_utt,\n",
    "                                 in_p='neutral', tran_p ='benign', out_p= 'toxic',\n",
    "                                 num_shot_con1= 6, num_shot_con2=6, num_shot_utt=6,\n",
    "                                 context = 'contextual scenario',  \n",
    "                                 qualifier_con1='highly positive', qualifier_con2='offensive',\n",
    "                                 do_sample=True, pa_con1=0.6, topk_con1=8, \n",
    "                                 pa_con2=0.6, topk_con2=8, pa_utt=0.6, topk_utt=8, \n",
    "                                 maxtoken_con1=70, maxtoken_con2 = 70, maxtoken_utt=40, \n",
    "                                 repetition_penalty=1.2, double_polarity= False, \n",
    "                                 multiple_utternace = False, multiple_utternace_no = None):\n",
    "    out = {}\n",
    "    data = dataset_index\n",
    "    for i in tqdm(range(len(data))):\n",
    "        key = data[i]\n",
    "        utterance = dataset[key]['text']\n",
    "        generation = augment_multistage(utterance, model_con1=model_con1, model_con2=model_con2, model_utt=model_utt, \n",
    "                                 tokenizer_con1=tokenizer_con1, tokenizer_con2=tokenizer_con2, tokenizer_utt=tokenizer_utt,\n",
    "                                 in_p=in_p, tran_p =tran_p, out_p= out_p,\n",
    "                                 num_shot_con1= num_shot_con1, num_shot_con2=num_shot_con2, num_shot_utt=num_shot_utt,\n",
    "                                 context = context,  \n",
    "                                 qualifier_con1=qualifier_con1, qualifier_con2=qualifier_con2,\n",
    "                                 do_sample=do_sample, pa_con1=pa_con1, topk_con1=topk_con1, \n",
    "                                 pa_con2=pa_con1, topk_con2=topk_con2, pa_utt=pa_utt, topk_utt=topk_utt, \n",
    "                                 maxtoken_con1=maxtoken_con1, maxtoken_con2 = maxtoken_con2, maxtoken_utt=maxtoken_utt, \n",
    "                                 repetition_penalty=repetition_penalty, double_polarity=double_polarity, \n",
    "                                 multiple_utternace = multiple_utternace, multiple_utternace_no = multiple_utternace_no)\n",
    "         \n",
    "        out[key] = generation\n",
    "            \n",
    "    return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa5490-70e9-4f30-9083-7f3dce112410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pipeline_augment_niter_multistage(\n",
    "    num_iteration,\n",
    "    dataset_index, \n",
    "    dataset, \n",
    "    model_con1, \n",
    "    model_con2, \n",
    "    model_utt, \n",
    "    tokenizer_con1, \n",
    "    tokenizer_con2, \n",
    "    tokenizer_utt, \n",
    "    stack_output = False,\n",
    "    in_p='neutral', \n",
    "    tran_p ='benign', \n",
    "    out_p= 'toxic',\n",
    "    num_shot_con1= 6,\n",
    "    num_shot_con2=6, \n",
    "    num_shot_utt=6,\n",
    "    context = 'contextual scenario',  \n",
    "    qualifier_con1='highly positive', \n",
    "    qualifier_con2='offensive',\n",
    "    do_sample=True, pa_con1=0.6, \n",
    "    topk_con1=8, \n",
    "    pa_con2=0.6, \n",
    "    topk_con2=8, \n",
    "    pa_utt=0.6, \n",
    "    topk_utt=8, \n",
    "    maxtoken_con1=70, \n",
    "    maxtoken_con2 = 70, \n",
    "    maxtoken_utt=40, \n",
    "    repetition_penalty=1.2, \n",
    "    double_polarity= False):\n",
    "    out = {}\n",
    "    data = dataset_index\n",
    "    for i in tqdm(range(len(data))):\n",
    "        key = data[i]\n",
    "        utterance = dataset[key]['text']\n",
    "        generation = augment_niter_multistage(num_iteration,utterance, model_con1=model_con1, model_con2=model_con2, model_utt=model_utt, \n",
    "                                 tokenizer_con1=tokenizer_con1, tokenizer_con2=tokenizer_con2, tokenizer_utt=tokenizer_utt, stack_output = stack_output,\n",
    "                                 in_p=in_p, tran_p =tran_p, out_p= out_p,\n",
    "                                 num_shot_con1= num_shot_con1, num_shot_con2=num_shot_con2, num_shot_utt=num_shot_utt,\n",
    "                                 context = context,  \n",
    "                                 qualifier_con1=qualifier_con1, qualifier_con2=qualifier_con2,\n",
    "                                 do_sample=do_sample, pa_con1=pa_con1, topk_con1=topk_con1, pa_con2=pa_con2, topk_con2=topk_con2, pa_utt=pa_utt, topk_utt=topk_utt, \n",
    "                                 maxtoken_con1=maxtoken_con1, maxtoken_con2 = maxtoken_con2, maxtoken_utt=maxtoken_utt, repetition_penalty=repetition_penalty, double_polarity= double_polarity)\n",
    "         \n",
    "        out[key] = generation\n",
    "            \n",
    "    return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84d799-d932-4605-897c-baf5d655d734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8088698-fd86-4eb8-bb87-8a47f4c69200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac4cf0-f7b6-4033-a899-07a975ff5b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
